use std::collections::HashMap;
use std::time::Duration;

use tracing::{debug, info, instrument, warn};

use super::error::ApiError;
use super::types::{Comment, Feed, HnItem, Story};
use crate::storage::{StorableComment, StorableStory, Storage};

const API_BASE: &str = "https://hacker-news.firebaseio.com/v0";
const PAGE_SIZE: usize = 30;

pub struct HnClient {
    http: reqwest::Client,
    storage: Option<Storage>,
}

impl HnClient {
    pub fn new(storage: Option<Storage>) -> Self {
        Self {
            http: reqwest::Client::builder()
                .timeout(Duration::from_secs(10))
                .build()
                .expect("Failed to create HTTP client"),
            storage,
        }
    }

    async fn get_json<T: serde::de::DeserializeOwned>(&self, url: &str) -> Result<T, ApiError> {
        let response = self.http.get(url).send().await?;
        let status = response.status();
        if !status.is_success() {
            warn!(status = %status, url, "http error");
            return Err(ApiError::HttpStatus(
                status.as_u16(),
                status.canonical_reason().unwrap_or("").into(),
            ));
        }
        response
            .json()
            .await
            .map_err(|e| ApiError::Parse(e.to_string()))
    }

    pub async fn fetch_feed_ids(&self, feed: Feed) -> Result<Vec<u64>, ApiError> {
        let url = format!("{}/{}.json", API_BASE, feed.endpoint());
        self.get_json(&url).await
    }

    async fn fetch_item(&self, id: u64) -> Result<HnItem, ApiError> {
        let url = format!("{}/item/{}.json", API_BASE, id);
        self.get_json(&url).await
    }

    #[instrument(skip(self), fields(feed = %feed.label(), page))]
    pub async fn fetch_stories(
        &self,
        feed: Feed,
        page: usize,
        force_refresh: bool,
    ) -> Result<Vec<Story>, ApiError> {
        info!("fetching stories");
        let ids = self.fetch_feed_ids(feed).await?;
        let start = page * PAGE_SIZE;
        let end = (start + PAGE_SIZE).min(ids.len());

        if start >= ids.len() {
            return Ok(vec![]);
        }

        let page_ids = &ids[start..end];
        let stories = self.fetch_stories_by_ids(page_ids, force_refresh).await?;
        info!(count = stories.len(), "fetched stories");
        Ok(stories)
    }

    pub async fn fetch_stories_by_ids(
        &self,
        ids: &[u64],
        force_refresh: bool,
    ) -> Result<Vec<Story>, ApiError> {
        let mut stories = Vec::with_capacity(ids.len());
        let mut to_fetch = Vec::new();

        // Check storage for cached stories (unless forcing refresh)
        if !force_refresh {
            if let Some(storage) = &self.storage {
                for &id in ids {
                    if let Ok(Some(cached)) = storage.get_fresh_story(id).await {
                        debug!(story_id = id, "cache hit");
                        stories.push(cached.into());
                    } else {
                        debug!(story_id = id, "cache miss");
                        to_fetch.push(id);
                    }
                }
            } else {
                to_fetch.extend_from_slice(ids);
            }
        } else {
            to_fetch.extend_from_slice(ids);
        }

        // Fetch remaining from API
        if !to_fetch.is_empty() {
            let futures: Vec<_> = to_fetch.iter().map(|&id| self.fetch_item(id)).collect();
            let results = futures::future::join_all(futures).await;

            let fetched: Vec<Story> = results
                .into_iter()
                .filter_map(|r| r.ok())
                .filter_map(Story::from_item)
                .collect();

            // Write-through to storage
            if let Some(storage) = &self.storage {
                for story in &fetched {
                    storage.save_story(&StorableStory::from(story)).await?;
                }
            }

            stories.extend(fetched);
        }

        // Re-sort by original id order
        let id_positions: HashMap<u64, usize> =
            ids.iter().enumerate().map(|(i, &id)| (id, i)).collect();
        stories.sort_by_key(|s| id_positions.get(&s.id).copied().unwrap_or(usize::MAX));

        Ok(stories)
    }

    /// Fetches comments using BFS for parallelism, then reorders to DFS for display
    #[instrument(skip(self, story), fields(story_id = story.id, max_depth))]
    pub async fn fetch_comments_flat(
        &self,
        story: &Story,
        max_depth: usize,
        force_refresh: bool,
    ) -> Result<Vec<Comment>, ApiError> {
        use std::collections::HashSet;

        info!("fetching comments");

        // Check storage for cached comments (unless forcing refresh)
        if !force_refresh
            && let Some(storage) = &self.storage
            && let Ok(Some(cached)) = storage.get_fresh_comments(story.id).await
        {
            info!(count = cached.len(), "comments cache hit");
            let comments: Vec<Comment> = cached.into_iter().map(|c| c.into()).collect();
            return Ok(order_cached_comments(comments, &story.kids));
        }

        let mut items: HashMap<u64, HnItem> = HashMap::new();
        let mut attempted: HashSet<u64> = HashSet::new();
        let mut to_fetch: Vec<u64> = story.kids.clone();
        let mut depth = 0;

        while !to_fetch.is_empty() && depth <= max_depth {
            let futures: Vec<_> = to_fetch.iter().map(|&id| self.fetch_item(id)).collect();
            let results = futures::future::join_all(futures).await;

            let mut next_fetch = Vec::new();
            for (id, result) in to_fetch.into_iter().zip(results) {
                attempted.insert(id);
                if let Ok(item) = result {
                    if item.deleted.unwrap_or(false) || item.dead.unwrap_or(false) {
                        continue;
                    }
                    if depth < max_depth {
                        next_fetch.extend(&item.kids);
                    }
                    items.insert(id, item);
                }
            }
            to_fetch = next_fetch;
            depth += 1;
        }

        let comments = build_comment_tree(items, &attempted, &story.kids);

        // Write-through to storage
        if let Some(storage) = &self.storage {
            let storable: Vec<StorableComment> = comments
                .iter()
                .map(|c| {
                    StorableComment::from_comment(c, story.id, find_parent_id(&comments, c.id))
                })
                .collect();
            storage.save_comments(story.id, &storable).await?;
        }

        info!(count = comments.len(), "fetched comments");
        Ok(comments)
    }
}

fn find_parent_id(comments: &[Comment], comment_id: u64) -> Option<u64> {
    for c in comments {
        if c.kids.contains(&comment_id) {
            return Some(c.id);
        }
    }
    None
}

/// Builds a DFS-ordered comment tree from fetched items.
///
/// Filters kids that were attempted but not fetched (deleted/dead), while
/// keeping kids that were never attempted (beyond max_depth) so UI shows
/// they have replies even if we can't display them.
pub fn build_comment_tree(
    mut items: HashMap<u64, HnItem>,
    attempted: &std::collections::HashSet<u64>,
    root_kids: &[u64],
) -> Vec<Comment> {
    let mut comments = Vec::new();
    let mut stack: Vec<(u64, usize)> = root_kids.iter().rev().map(|&id| (id, 0)).collect();

    while let Some((id, depth)) = stack.pop() {
        if let Some(mut item) = items.remove(&id) {
            item.kids
                .retain(|kid_id| !attempted.contains(kid_id) || items.contains_key(kid_id));

            for &kid_id in item.kids.iter().rev() {
                stack.push((kid_id, depth + 1));
            }
            if let Some(comment) = Comment::from_item(item, depth) {
                comments.push(comment);
            }
        }
    }

    comments
}

/// Orders cached comments into DFS tree order using stored kids arrays.
fn order_cached_comments(cached: Vec<Comment>, root_kids: &[u64]) -> Vec<Comment> {
    let mut by_id: HashMap<u64, Comment> = cached.into_iter().map(|c| (c.id, c)).collect();
    let mut result = Vec::with_capacity(by_id.len());
    let mut stack: Vec<u64> = root_kids.iter().rev().copied().collect();

    while let Some(id) = stack.pop() {
        if let Some(comment) = by_id.remove(&id) {
            for &kid_id in comment.kids.iter().rev() {
                stack.push(kid_id);
            }
            result.push(comment);
        }
    }

    result
}

impl Default for HnClient {
    fn default() -> Self {
        Self::new(None)
    }
}

impl Clone for HnClient {
    fn clone(&self) -> Self {
        Self {
            http: self.http.clone(),
            storage: self.storage.clone(),
        }
    }
}

impl Clone for HnItem {
    fn clone(&self) -> Self {
        Self {
            id: self.id,
            item_type: self.item_type.clone(),
            by: self.by.clone(),
            time: self.time,
            text: self.text.clone(),
            url: self.url.clone(),
            score: self.score,
            title: self.title.clone(),
            descendants: self.descendants,
            kids: self.kids.clone(),
            parent: self.parent,
            deleted: self.deleted,
            dead: self.dead,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashSet;

    fn make_comment_item(id: u64, by: &str, text: &str, kids: Vec<u64>) -> HnItem {
        HnItem {
            id,
            item_type: Some("comment".to_string()),
            by: Some(by.to_string()),
            time: Some(1700000000),
            text: Some(text.to_string()),
            url: None,
            score: None,
            title: None,
            descendants: None,
            kids,
            parent: None,
            deleted: None,
            dead: None,
        }
    }

    #[tokio::test]
    async fn test_client_creation() {
        let client = HnClient::new(None);
        // Just verify it doesn't panic
        drop(client);
    }

    /// Verifies that deleted children (attempted but not fetched) are filtered
    /// out of the kids array.
    #[test]
    fn test_deleted_children_filtered_from_kids() {
        let mut items: HashMap<u64, HnItem> = HashMap::new();
        let mut attempted: HashSet<u64> = HashSet::new();

        // Parent comment with kids [2, 3] - child 3 was attempted but deleted
        items.insert(
            1,
            make_comment_item(1, "parent", "Parent comment", vec![2, 3]),
        );
        items.insert(2, make_comment_item(2, "child", "Child comment", vec![]));

        // Both children were attempted, but child 3 was deleted (not in items)
        attempted.insert(2);
        attempted.insert(3);

        let comments = build_comment_tree(items, &attempted, &[1]);

        assert_eq!(comments.len(), 2);
        let parent = &comments[0];
        assert_eq!(parent.id, 1);
        assert_eq!(parent.kids, vec![2]);
    }

    /// Verifies that a comment whose only child was deleted ends up with
    /// an empty kids array (showing [ ] instead of [+] in the UI).
    #[test]
    fn test_all_children_deleted_results_in_empty_kids() {
        let mut items: HashMap<u64, HnItem> = HashMap::new();
        let mut attempted: HashSet<u64> = HashSet::new();

        items.insert(
            1,
            make_comment_item(1, "author", "Comment with deleted reply", vec![999]),
        );

        // Child 999 was attempted but deleted (not in items)
        attempted.insert(999);

        let comments = build_comment_tree(items, &attempted, &[1]);

        assert_eq!(comments.len(), 1);
        assert!(comments[0].kids.is_empty());
    }

    /// Verifies that children beyond max_depth (never attempted) are kept in
    /// the kids array so the UI shows they have replies.
    #[test]
    fn test_children_beyond_max_depth_kept_in_kids() {
        let mut items: HashMap<u64, HnItem> = HashMap::new();
        let attempted: HashSet<u64> = HashSet::new();

        // Comment at max_depth with a child that was never fetched
        items.insert(
            1,
            make_comment_item(1, "deep_commenter", "Comment at max depth", vec![999]),
        );

        // Child 999 was NOT attempted (beyond max_depth)
        let comments = build_comment_tree(items, &attempted, &[1]);

        assert_eq!(comments.len(), 1);
        assert_eq!(comments[0].kids, vec![999]);
    }
}
